{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "# from skimage.io import imread\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dropout,Flatten,Dense\n",
    "from tensorflow.keras.activations import relu,linear \n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data files path settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_path = \"/Users/devasenan/Documents/conser-vision/data/\"\n",
    "root_path=\"D:/Course/DrivenData Project/Dataset/\"\n",
    "# root_path = \"C:/Users/devas/Documents/projects/conser-vision/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Train features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>filepath</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZJ000000</td>\n",
       "      <td>train_features/ZJ000000.jpg</td>\n",
       "      <td>S0120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZJ000001</td>\n",
       "      <td>train_features/ZJ000001.jpg</td>\n",
       "      <td>S0069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZJ000002</td>\n",
       "      <td>train_features/ZJ000002.jpg</td>\n",
       "      <td>S0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZJ000003</td>\n",
       "      <td>train_features/ZJ000003.jpg</td>\n",
       "      <td>S0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZJ000004</td>\n",
       "      <td>train_features/ZJ000004.jpg</td>\n",
       "      <td>S0036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                     filepath   site\n",
       "0  ZJ000000  train_features/ZJ000000.jpg  S0120\n",
       "1  ZJ000001  train_features/ZJ000001.jpg  S0069\n",
       "2  ZJ000002  train_features/ZJ000002.jpg  S0009\n",
       "3  ZJ000003  train_features/ZJ000003.jpg  S0008\n",
       "4  ZJ000004  train_features/ZJ000004.jpg  S0036"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features = pd.read_csv(root_path+\"train_features.csv\")\n",
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = []\n",
    "size = 128\n",
    "length = 487\n",
    "train_sites = []\n",
    "\n",
    "def hist(img):\n",
    "  img_to_yuv = cv2.cvtColor(img,cv2.COLOR_BGR2YUV)\n",
    "  img_to_yuv[:,:,0] = cv2.equalizeHist(img_to_yuv[:,:,0])\n",
    "  hist_equalization_result = cv2.cvtColor(img_to_yuv, cv2.COLOR_YUV2BGR)\n",
    "  return hist_equalization_result\n",
    "\n",
    "# img=cv2.imread(root_path+train_features.iloc[0,1])\n",
    "# # img=cv2.cvtColor(img,cv2.COLOR_GRAY2BGR)\n",
    "# img=ndimage.gaussian_filter(img,sigma=0.2)\n",
    "# img=hist(img)\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(length):\n",
    "    img=cv2.imread(root_path+train_features.iloc[i, 1])\n",
    "    img = cv2.resize(img, (size, size))\n",
    "    img=ndimage.gaussian_filter(img,sigma=0.3)\n",
    "    img=hist(img)\n",
    "    img = np.array(img).flatten()\n",
    "    train_imgs.append(img)\n",
    "    train_sites.append(train_features.iloc[i, 2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = np.array(train_imgs)\n",
    "train_sites = np.array(train_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(487, 49152)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = train_imgs/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89411765, 0.89411765, 0.89411765, ..., 0.97254902, 0.97254902,\n",
       "        0.97254902],\n",
       "       [0.87058824, 0.87058824, 0.87058824, ..., 0.98039216, 0.98039216,\n",
       "        0.98039216],\n",
       "       [0.76862745, 0.76862745, 0.76862745, ..., 0.97254902, 0.97254902,\n",
       "        0.97254902],\n",
       "       ...,\n",
       "       [0.56470588, 0.6627451 , 0.52941176, ..., 0.98823529, 0.98823529,\n",
       "        0.98823529],\n",
       "       [0.2745098 , 0.2745098 , 0.2745098 , ..., 0.91764706, 0.91764706,\n",
       "        0.91764706],\n",
       "       [0.04705882, 0.04705882, 0.04705882, ..., 0.65098039, 0.65098039,\n",
       "        0.65098039]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read train labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lbls = pd.read_csv(root_path+\"train_labels.csv\")\n",
    "train_lbls = np.array(train_lbls.iloc[:487, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lbls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>filepath</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZJ016488</td>\n",
       "      <td>test_features/ZJ016488.jpg</td>\n",
       "      <td>S0082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZJ016489</td>\n",
       "      <td>test_features/ZJ016489.jpg</td>\n",
       "      <td>S0040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZJ016490</td>\n",
       "      <td>test_features/ZJ016490.jpg</td>\n",
       "      <td>S0040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZJ016491</td>\n",
       "      <td>test_features/ZJ016491.jpg</td>\n",
       "      <td>S0041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZJ016492</td>\n",
       "      <td>test_features/ZJ016492.jpg</td>\n",
       "      <td>S0040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                    filepath   site\n",
       "0  ZJ016488  test_features/ZJ016488.jpg  S0082\n",
       "1  ZJ016489  test_features/ZJ016489.jpg  S0040\n",
       "2  ZJ016490  test_features/ZJ016490.jpg  S0040\n",
       "3  ZJ016491  test_features/ZJ016491.jpg  S0041\n",
       "4  ZJ016492  test_features/ZJ016492.jpg  S0040"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = pd.read_csv(root_path+\"test_features.csv\")\n",
    "test_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4464, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = []\n",
    "size = 128\n",
    "length = 464\n",
    "test_sites = []\n",
    "\n",
    "for i in range(length):\n",
    "    img=cv2.imread(root_path+test_features.iloc[i, 1],-1)\n",
    "    img = cv2.resize(img, (size, size))\n",
    "    img=ndimage.gaussian_filter(img,sigma=0.3)\n",
    "#     img=hist(img)\n",
    "    img=np.array(img).flatten()\n",
    "    test_imgs.append(img)\n",
    "    test_sites.append(test_features.iloc[i, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = np.array(test_imgs)\n",
    "test_sites = np.array(test_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = test_imgs/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 6,\n",
       " 7,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 0,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 7]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = []\n",
    "\n",
    "for i in train_lbls: \n",
    "    train_y.append(list(i).index(1))\n",
    "\n",
    "train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FEATURE EXTRACTOR function\n",
    "# #input shape is (n, x, y, c) number of images, x, y, and channels\n",
    "# def feature_extractor(dataset):\n",
    "#     x_train =  dataset\n",
    "#     image_dataset = pd.DataFrame()\n",
    "#     for image in range(x_train.shape[0]):  # iterate through each file\n",
    "#         #print(image)\n",
    "#         df = pd.DataFrame()  # Temporary data frame to capture information for e\n",
    "#         #Reset dataframe to blank after each Loop.\n",
    "#         input_img = x_train[image, :]\n",
    "#         img = input_img\n",
    "#         ########\n",
    "#         #START ADDING DATA TO THE DATAFRAME\n",
    "#         #Add feature extractors, e.g. edge detection, smoothing, etc.\n",
    "#         # FEATURE 1 Pixel values\n",
    "#         #Add pixel values to the data frame\n",
    "#         pixel_values = img.reshape(-1)\n",
    "#         df['Pixel_Value'] = pixel_values  # Pixel value itself as a feature\n",
    "#         #df['Image_Name'] = image #Capture image name as we read multiple ima\n",
    "       \n",
    "#         # FEATURE 2 Bunch of Gabor filter responses\n",
    "#         #Generate Gabor features\n",
    "#         num = 1  # To count numbers up in order to give Gabor features a Lable {\n",
    "#         kernels = []\n",
    "#         for theta in range(2):  # Define number of thetas\n",
    "#             theta = theta / 4. * np.pi\n",
    "#             for sigma in (1, 3):  # Sigma with 1 and\n",
    "#                 lamda = np.pi/4\n",
    "#                 gamma = 0.5\n",
    "#                 gabor_label = 'Gabor' + str(num)  # Label Gabor columns as Gabor\n",
    "#                 print(gabor_label)\n",
    "#                 ksize = 9\n",
    "#                 kernel = cv2.getGaborKernel((ksize, ksize), sigma, theta, lamda, gamma)\n",
    "#                 kernels.append(kernel)\n",
    "                                        \n",
    "#                 fimg = cv2.filter2D(img, cv2.CV_8UC3, kernel)\n",
    "#                 filtered_img = fimg.reshape(-1)\n",
    "#                 # Labels columns as Gabor1, Gabc\n",
    "#                 df[gabor_label] = filtered_img\n",
    "#                 print(gabor_label, ': theta=', theta, ': sigma=', sigma, ': lamda=', lamda)\n",
    "#                 num += 1\n",
    "\n",
    "\n",
    "#         image_dataset=image_dataset.append(df)\n",
    "#     return image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer conv2d_21 is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: [None, 49152]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13732\\3528408685.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     ])\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, layers, name)\u001b[0m\n\u001b[0;32m    108\u001b[0m       \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_no_legacy_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    172\u001b[0m           \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m           \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m           \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m           \u001b[0mset_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;31m# the corresponding TF subgraph inside `backend.get_graph()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m         input_spec.assert_input_compatibility(self.input_spec, inputs,\n\u001b[1;32m--> 586\u001b[1;33m                                               self.name)\n\u001b[0m\u001b[0;32m    587\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m                          \u001b[1;34m'expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. Full shape received: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                          str(x.shape.as_list()))\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m       \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer conv2d_21 is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: [None, 49152]"
     ]
    }
   ],
   "source": [
    "model =Sequential(\n",
    "    [\n",
    "        Conv2D(16,(5,5),input_shape=train_imgs.shape[1:],activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2,2),strides=2,padding='same'),\n",
    "        MaxPooling2D(pool_size=(2,2),strides=2,padding='same'),\n",
    "        Dropout(0.1),\n",
    "        Conv2D(32,(5,5),activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2,2),strides=2,padding='same'),\n",
    "        Dropout(0.1),\n",
    "        Conv2D(64,(5,5),activation='relu'),\n",
    "        Dropout(0.8),\n",
    "        Flatten(),\n",
    "        Dense(2,activation='linear')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13732\\1401115294.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_feat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dense_2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfeat_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_imgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "model_feat=Model(inputs=model.input,outputs=model.get_layer('dense_2').output)\n",
    "feat_train=model_feat.predict(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Extract features from training images\n",
    "# print(train_imgs)\n",
    "# image_features = feature_extractor(train_imgs)\n",
    "# #Reshape to a vector for Random Forest /SVM training\n",
    "# n_features = image_features.shape[1]\n",
    "# image_features = np.expand_dims(image_features, axis=0)\n",
    "# X_for_RF = np.reshape(image_features, (train_imgs.shape[0], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf=RandomForestClassifier(n_estimators = 50, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_imgs, img_x, img_y, img_z = train_imgs.shape\n",
    "# train_imgs_reshaped = train_imgs.reshape(n_imgs, img_x*img_y*img_z)\n",
    "# train_imgs_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=50, random_state=42)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(feat_test,np.argmax(train_y,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf.score(feat_test,np.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_imgs, img_x, img_y, img_z = test_imgs.shape\n",
    "# test_imgs_reshaped = test_imgs.reshape(n_imgs, img_x*img_y*img_z)\n",
    "# test_imgs_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n\u001b[1;32m----> 5\u001b[0m test_features \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_imgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m test_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(test_features, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m test_for_RF \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(test_features, (test_imgs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mfeature_extractor\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()  \u001b[38;5;66;03m# Temporary data frame to capture information for e\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#Reset dataframe to blank after each Loop.\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m input_img \u001b[38;5;241m=\u001b[39m \u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     11\u001b[0m img \u001b[38;5;241m=\u001b[39m input_img\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m########\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#START ADDING DATA TO THE DATAFRAME\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#Add feature extractors, e.g. edge detection, smoothing, etc.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# FEATURE 1 Pixel values\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#Add pixel values to the data frame\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "#Predict on Test data\n",
    "#Extract features from test data and reshape, just like training data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "test_features = feature_extractor(test_imgs)\n",
    "test_features = np.expand_dims(test_features, axis=0)\n",
    "test_for_RF = np.reshape(test_features, (test_imgs.shape[0], -1))\n",
    "#Predict on test\n",
    "test_prediction = model.predict(test_for_RF)\n",
    "test_prediction\n",
    "\n",
    "\n",
    "# #Print overall accuracy\n",
    "# print(\"Accuracy = \", metrics.accuracy_score(test_labels, test_prediction))\n",
    "# #Print confusion matrix\n",
    "# cm = confusion_matrix(test_labels, test_prediction)\n",
    "# fig, ax = plt.subplots(figsize=(6, 6))  # Sample figsize in inches\n",
    "# sns.set(font_scale=1.6)\n",
    "# sns.heatmap(cm, annot-True, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 7, 7, 5, 6, 6, 5, 6, 0, 0, 2, 0, 0, 7, 3, 3, 0, 2, 0, 6, 7, 0,\n",
       "       7, 5, 5, 0, 7, 0, 7, 0, 7, 6, 7, 2, 7, 6, 5, 5, 6, 3, 6, 6, 0, 7,\n",
       "       6, 0, 7, 7, 6, 5, 6, 5, 6, 2, 7, 7, 6, 6, 5, 7, 7, 2, 0, 0, 7, 6,\n",
       "       0, 6, 2, 6, 5, 2, 0, 6, 0, 2, 0, 5, 2, 5, 5, 6, 0, 0, 6, 5, 7, 7,\n",
       "       7, 5, 0, 0, 6, 6, 6, 2, 0, 5, 0, 2, 7, 5, 0, 2, 0, 0, 2, 5, 0, 1,\n",
       "       6, 0, 1, 0, 6, 6, 7, 0, 0, 3, 7, 0, 2, 6, 7, 5, 7, 6, 0, 0, 0, 7,\n",
       "       7, 7, 6, 7, 6, 3, 7, 6, 0, 3, 6, 7, 7, 5, 0, 0, 5, 0, 0, 7, 6, 7,\n",
       "       7, 0, 0, 6, 6, 7, 0, 7, 2, 2, 5, 6, 6, 1, 0, 6, 5, 0, 0, 3, 2, 2,\n",
       "       5, 6, 6, 6, 2, 2, 1, 5, 5, 1, 5, 2, 0, 7, 5, 6, 6, 0, 0, 6, 7, 5,\n",
       "       6, 2, 6, 0, 7, 5, 2, 0, 6, 2, 0, 2, 7, 7, 6, 1, 5, 6, 0, 6, 7, 0,\n",
       "       5, 0, 6, 6, 0, 6, 3, 7, 5, 5, 5, 0, 0, 6, 6, 7, 2, 2, 0, 0, 7, 6,\n",
       "       7, 2, 0, 6, 6, 5, 0, 6, 6, 6, 7, 0, 6, 0, 7, 0, 7, 2, 6, 5, 1, 2,\n",
       "       6, 6, 5, 2, 5, 6, 2, 7, 6, 0, 7, 0, 6, 0, 6, 2, 0, 5, 0, 7, 0, 6,\n",
       "       5, 0, 3, 6, 5, 0, 5, 0, 5, 1, 0, 2, 5, 0, 0, 1, 0, 0, 6, 0, 7, 7,\n",
       "       5, 2, 0, 1, 6, 6, 6, 6, 7, 7, 3, 6, 0, 7, 3, 0, 6, 6, 5, 0, 0, 6,\n",
       "       7, 6, 6, 3, 2, 1, 5, 4, 2, 0, 7, 3, 5, 2, 5, 0, 0, 6, 0, 0, 0, 2,\n",
       "       6, 0, 6, 0, 6, 7, 2, 0, 0, 7, 7, 5, 6, 6, 0, 5, 7, 7, 0, 7, 5, 7,\n",
       "       6, 6, 6, 5, 6, 2, 0, 6, 5, 0, 5, 0, 2, 5, 5, 5, 7, 7, 1, 5, 1, 0,\n",
       "       5, 6, 0, 6, 7, 0, 7, 0, 0, 3, 5, 0, 0, 0, 7, 0, 5, 0, 0, 6, 3, 5,\n",
       "       6, 6, 5, 0, 0, 0, 5, 5, 3, 0, 5, 6, 0, 6, 5, 7, 2, 3, 6, 1, 3, 6,\n",
       "       6, 0, 0, 2, 7, 6, 5, 0, 6, 6, 7, 0, 3, 7, 7, 5, 0, 2, 1, 2, 5, 5,\n",
       "       5, 7])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=pd.read_csv(root_path+\"submission_format.csv\")\n",
    "submission = submission.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = []\n",
    "k=0\n",
    "for i in range(submission.shape[0]):\n",
    "    k=np.argmax(submission.iloc[i,:])\n",
    "    test_y.append(k)\n",
    "test_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 6, 2, ..., 2, 2, 6], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11206896551724138"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(test_y[:464], test_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "b6cfd2e89d76bb44000c5d3ac5497f72d20617ef578fef45567e766b8515e50c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
